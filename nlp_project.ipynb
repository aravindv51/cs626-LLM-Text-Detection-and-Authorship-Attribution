{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVzngUNQfAdN"
      },
      "outputs": [],
      "source": [
        "#! pip install transformers torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cpBUYoFfD9H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = getToken()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGkwtRJvLr8j",
        "outputId": "9e5a384b-133d-4440-b86e-8ce16b6e9135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7bxBxrxnk6k"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from collections import Counter\n",
        "import re\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwkIBKMrnfJz",
        "outputId": "1e61da92-1f90-4000-b9e0-8c98ff00d112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading model or classifier: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
            "Model and classifier loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch import nn\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoModel\n",
        "import os\n",
        "import threading\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "class WhosAIClassifier:\n",
        "    def __init__(self, embedder, tokenizer, device):\n",
        "        self.embedder = embedder\n",
        "        self.multiclass_centroids = None\n",
        "        self.binary_centroids = None\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def _get_embeddings(self, texts, features, batch_size=32):\n",
        "        embeddings_list = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_features = features[i:i + batch_size]\n",
        "            encoding = self.tokenizer(\n",
        "                batch_texts,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids = encoding['input_ids'].to(self.device)\n",
        "            attention_mask = encoding['attention_mask'].to(self.device)\n",
        "            batch_features_tensor = torch.tensor(batch_features, dtype=torch.float32).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.embedder(input_ids, attention_mask, batch_features_tensor).detach().cpu().numpy()\n",
        "            embeddings_list.append(embeddings)\n",
        "        return np.concatenate(embeddings_list, axis=0)\n",
        "\n",
        "    def fit(self, texts, labels, features,batch_size=32):\n",
        "        binary_labels = [1 if label == 'human' else 0 for label in labels]\n",
        "        embeddings = self._get_embeddings(texts, features,batch_size)\n",
        "        self.multiclass_centroids = NearestCentroid().fit(embeddings, labels)\n",
        "        self.binary_centroids = NearestCentroid().fit(embeddings, binary_labels)\n",
        "        return self\n",
        "\n",
        "    def predict(self, texts, features, batch_size=16):\n",
        "        predictions_multi = []\n",
        "        predictions_binary = []\n",
        "\n",
        "        chunk_size = 1000\n",
        "        for i in range(0, len(texts), chunk_size):\n",
        "            chunk_end = min(i + chunk_size, len(texts))\n",
        "            chunk_texts = texts[i:chunk_end]\n",
        "            chunk_features = features[i:chunk_end]\n",
        "\n",
        "            embeddings = self._get_embeddings(chunk_texts, chunk_features, batch_size)\n",
        "            chunk_pred_multi = self.multiclass_centroids.predict(embeddings)\n",
        "            chunk_pred_binary = self.binary_centroids.predict(embeddings)\n",
        "\n",
        "            predictions_multi.extend(chunk_pred_multi)\n",
        "            predictions_binary.extend(chunk_pred_binary)\n",
        "\n",
        "            del embeddings\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return np.array(predictions_multi), np.array(predictions_binary)\n",
        "\n",
        "\n",
        "    def save(self, path=\"classifier.joblib\"):\n",
        "        joblib.dump(self, path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path=\"classifier.joblib\"):\n",
        "        return joblib.load(path)\n",
        "\n",
        "class AuthorAttributionModel(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "        self.uid_weight_net = nn.Sequential(\n",
        "            nn.Linear(5, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 128),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, uid_features):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        text_embeddings = self.text_projection(text_embeddings)\n",
        "        text_embeddings = nn.functional.normalize(text_embeddings, p=2, dim=1)\n",
        "\n",
        "        uid_weights = self.uid_weight_net(uid_features)\n",
        "\n",
        "        weighted_embeddings = text_embeddings * uid_weights\n",
        "        final_embeddings = nn.functional.normalize(weighted_embeddings, p=2, dim=1)\n",
        "\n",
        "        return final_embeddings\n",
        "\n",
        "\n",
        "\n",
        "class InferenceHelper:\n",
        "    def __init__(self, model_path=\"author_attribution_model1.pt\", classifier_path=\"whosai_classifier_1.joblib\"):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.model = AuthorAttributionModel()\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=True))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.classifier = joblib.load(classifier_path)\n",
        "        self.classifier.embedder = self.model\n",
        "        self.classifier.device = self.device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def get_embedding(self, text,features):\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "        features = features.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model(input_ids, attention_mask,features)\n",
        "\n",
        "        return embedding.cpu().numpy()\n",
        "\n",
        "    def predict_single(self, text,features):\n",
        "        embedding = self.get_embedding(text,features)\n",
        "        multiclass_pred = self.classifier.multiclass_centroids.predict(embedding)\n",
        "        binary_pred = self.classifier.binary_centroids.predict(embedding)\n",
        "        binary_label = \"human\" if binary_pred[0] == 1 else \"non-human\"\n",
        "\n",
        "        return {\n",
        "            \"multiclass_prediction\": multiclass_pred[0],\n",
        "            \"binary_prediction\": binary_label,\n",
        "            \"embedding\": embedding[0]\n",
        "        }\n",
        "    def predict_without_features(self,text):\n",
        "        features = gptwho_get_features(text)\n",
        "        features = torch.tensor(features,dtype=torch.float32)\n",
        "        return self.predict_single(text,features)\n",
        "\n",
        "\n",
        "\n",
        "def Inference(model_path,classifier_path):\n",
        "    try:\n",
        "        inference = InferenceHelper(model_path,classifier_path)\n",
        "        print(\"Model and classifier loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model or classifier: {e}\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nEnter a text to classify (or 'quit' to exit):\")\n",
        "        text = input()\n",
        "\n",
        "        if text.lower() == 'quit':\n",
        "            break\n",
        "        features = gptwho_get_features(text)\n",
        "        features = torch.tensor(features,dtype=torch.float32)\n",
        "        try:\n",
        "            predictions = inference.predict_single(text,features)\n",
        "            print(\"\\nPredictions:\")\n",
        "            print(f\"Detailed class: {predictions['multiclass_prediction']}\")\n",
        "            print(f\"Binary class: {predictions['binary_prediction']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error making prediction: {e}\")\n",
        "\n",
        "\n",
        "def evaluate_whosai(inference_helper, test_csv_path, batch_size=16):\n",
        "\n",
        "    print(\"Loading test data...\")\n",
        "    data = pd.read_csv('uid_features_test.csv')\n",
        "    texts = data['text'].tolist()\n",
        "    features = data[['uid_var', 'uid_diff', 'uid_diff2', 'mean', 'sum']].values\n",
        "    true_labels = data['label'].tolist()\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    multiclass_pred, binary_pred = inference_helper.classifier.predict(texts, features, batch_size=batch_size)\n",
        "    binary_pred_labels = ['human' if pred == 1 else 'non-human' for pred in binary_pred]\n",
        "    binary_true_labels = ['human' if label == 'human' else 'non-human' for label in true_labels]\n",
        "\n",
        "    print(\"\\nMulticlass Classification Report:\")\n",
        "    print(classification_report(true_labels, multiclass_pred))\n",
        "\n",
        "    print(\"\\nBinary Classification Report (Human vs Non-Human):\")\n",
        "    print(classification_report(binary_true_labels, binary_pred_labels))\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Text': texts,\n",
        "        'True_Label': true_labels,\n",
        "        'Predicted_Multiclass': multiclass_pred,\n",
        "        'Predicted_Binary': binary_pred_labels\n",
        "    })\n",
        "    results_df.to_csv('evaluation_results_2.csv', index=False)\n",
        "    plot_confusion_matrices(true_labels, multiclass_pred, binary_true_labels, binary_pred_labels)\n",
        "    print(\"\\nDetailed results saved to 'evaluation_results.csv'\")\n",
        "\n",
        "def Test():\n",
        "    try:\n",
        "        # Set memory management parameters\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "\n",
        "        model_path, classifier_path = \"/content/drive/MyDrive/Whos AI/Final Results/author_attribution_model.pt\", \"/content/drive/MyDrive/Whos AI/Final Results/whosai_classifier.joblib\"\n",
        "        Inference(model_path, classifier_path)\n",
        "        print(\"Model and classifier loaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFzB7jm9NPUv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5aZ0cZKow2X",
        "outputId": "becaf5d1-cfea-4c9a-ffa9-828f04e935d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dataset  'Final Results'  'Other Results'   Papers\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/MyDrive/Whos AI/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RCeKPGcKMuLO",
        "outputId": "45248a5f-f253-4704-bdad-eb611a157b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.6.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.3 (from gradio)\n",
            "  Downloading gradio_client-1.4.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.3->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.6.0-py3-none-any.whl (57.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.3-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.8.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.5 ffmpy-0.4.0 gradio-5.6.0 gradio-client-1.4.3 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.8.0 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0 uvicorn-0.32.1 websockets-12.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "408ab85660ce45d3afec4d600aaaee95",
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "bb3YXJ7AMYiV",
        "outputId": "48ff9123-9d76-4e2b-951c-98f8e8e2dbd7"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 2465",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-52fbe9ae4a0f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 2465"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "df = pd.read_csv(\"test.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rx_uAf8KixO"
      },
      "outputs": [],
      "source": [
        "global feature_list = []\n",
        "\n",
        "def get_random_row(category):\n",
        "    global feature_list = []\n",
        "    category_data = df[df['label'] == category]\n",
        "    if not category_data.empty:\n",
        "        random_row = random.choice(category_data['Generation'].tolist())\n",
        "        return random_row\n",
        "    return \"No data available for this category.\"\n",
        "\n",
        "def process_output(input_context):\n",
        "  try:\n",
        "    model_path = \"/content/drive/MyDrive/Whos AI/experiment 3/author_attribution_model1.pt\"\n",
        "    classifier_path = \"/content/drive/MyDrive/Whos AI/experiment 3/whosai_classifier_1.joblib\"\n",
        "    inference = InferenceHelper(model_path,classifier_path)\n",
        "    predictions = inference.predict_single(input_context,features)\n",
        "    result = f\"Detailed class: {predictions['multiclass_prediction']}\" + \"\\n\" + f\"Binary class: {predictions['binary_prediction']}\"\n",
        "    return result\n",
        "  except Exception as e:\n",
        "        print(f\"Error loading model or classifier: {e}\")\n",
        "\n",
        "def create_gradio_interface():\n",
        "    with gr.Blocks() as app:\n",
        "        # Dropdown to select a category\n",
        "        category_dropdown = gr.Dropdown(choices=df['label'].unique().tolist(), label=\"Select LLM\")\n",
        "\n",
        "        # Submit button\n",
        "        submit_button = gr.Button(\"Get generation Context\")\n",
        "\n",
        "        # Textbox to display the output\n",
        "        output_textbox = gr.Textbox(label=\"Random Output\")\n",
        "\n",
        "        # Define action on button click\n",
        "        submit_button.click(\n",
        "            fn=get_random_row,\n",
        "            inputs=category_dropdown,  # Pass dropdown value as input\n",
        "            outputs=output_textbox  # Output to the textbox\n",
        "        )\n",
        "\n",
        "        submit_button_2 = gr.Button(\"Process Output\")\n",
        "\n",
        "        processed_output_textbox = gr.Textbox(label=\"Processed Output\", interactive=False)\n",
        "\n",
        "        submit_button_2.click(\n",
        "            fn=process_output,  # Call the function to process the output\n",
        "            inputs=output_textbox,  # Pass the output of the first textbox as input\n",
        "            outputs=processed_output_textbox  # Output to the second textbox\n",
        "        )\n",
        "\n",
        "    return app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSAfYFRnira"
      },
      "outputs": [],
      "source": [
        "def gptwho_get_features(sentence):\n",
        "    device = torch.device('cuda')\n",
        "    print(\"CUDA status:\", torch.cuda.is_available())\n",
        "\n",
        "    all_sents = sentence\n",
        "\n",
        "    def local_diff(x):\n",
        "        d = 0\n",
        "        for i in range(len(x)-1):\n",
        "            d += abs(x[i+1]-x[i])\n",
        "        return d/len(x)\n",
        "\n",
        "    def local_diff2(x):\n",
        "        d = 0\n",
        "        for i in range(len(x)-1):\n",
        "            d += (x[i+1]-x[i])**2\n",
        "        return d/len(x)\n",
        "\n",
        "\n",
        "\n",
        "    tokenizer_class, model_class = GPT2Tokenizer, GPT2LMHeadModel\n",
        "    tokenizer = tokenizer_class.from_pretrained(\"gpt2-xl\")\n",
        "    model = model_class.from_pretrained(\"gpt2-xl\").to(device)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Getting UID_var and all surprisals for \"UID spans\" features\n",
        "    def get_line_uid_surp(lines):\n",
        "        with torch.no_grad():\n",
        "            lines = tokenizer.eos_token + lines\n",
        "            tok_res = tokenizer(lines, return_tensors='pt')\n",
        "            input_ids = tok_res['input_ids'][0].to(device)\n",
        "            attention_mask = tok_res['attention_mask'][0].to(device)\n",
        "            lines_len = torch.sum(tok_res['attention_mask'], dim=1)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "            loss, logits = outputs[:2]\n",
        "        line_log_prob = torch.tensor([0.0])\n",
        "        word_probs = []\n",
        "        for token_ind in range(lines_len - 1):\n",
        "            token_prob = F.softmax(logits[token_ind], dim=0).to('cpu')\n",
        "            token_id = input_ids[token_ind + 1].to('cpu')\n",
        "            target_log_prob = -torch.log(token_prob[token_id]).detach().cpu().numpy()\n",
        "            line_log_prob += -torch.log(token_prob[token_id]).detach().cpu().numpy()\n",
        "            word_probs.append(target_log_prob)\n",
        "\n",
        "        # mu = average surprisal\n",
        "        mu = line_log_prob/(lines_len-1)\n",
        "        uid = torch.tensor([0.0])\n",
        "\n",
        "        for i in range(len(word_probs)):\n",
        "            uid += (word_probs[i] - mu)**2/(len(word_probs))\n",
        "        sentence_uids = uid.detach().cpu().numpy()[0]\n",
        "        sentence_surprisal = np.mean(word_probs)\n",
        "        sentence_length = lines_len.detach().cpu().numpy()[0]\n",
        "        torch.cuda.empty_cache()\n",
        "        return(sentence_uids, sentence_surprisal, word_probs, sentence_length)\n",
        "\n",
        "    #row=[\"text\", \"label\", \"uid_var\", \"uid_diff\", \"uid_diff2\", \"mean\", \"sum\", \"surps\", \"n_token\"]\n",
        "\n",
        "    uids, surps, probs, lens = get_line_uid_surp(sentence)\n",
        "    uid_diff1 = local_diff(probs)\n",
        "    uid_diff2 = local_diff2(probs)\n",
        "    sum = np.sum(probs)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    #row_=[batch, labels, uids, uid_diff1, uid_diff2, surps, sum, probs, lens]\n",
        "    #print(row_)\n",
        "    features = [uids,uid_diff1,uid_diff2,surps,sum]\n",
        "    return features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuzvfM1wkR03"
      },
      "outputs": [],
      "source": [
        "# import gradio as gr\n",
        "# import pandas as pd\n",
        "# import random\n",
        "\n",
        "# # Load the dataset\n",
        "# df = pd.read_csv(\"test.csv\")  # Ensure test.csv contains 'sentence' and 'label' columns\n",
        "\n",
        "# print(df.head())\n",
        "\n",
        "# # Function to fetch a random sentence for a selected LLM\n",
        "# def get_random_sentence(llm_name):\n",
        "#     llm_data = df[df[\"label\"] == llm_name]\n",
        "#     if llm_data.empty:\n",
        "#         return \"No data available for the selected LLM.\"\n",
        "#     return random.choice(llm_data[\"Generation\"].tolist())\n",
        "\n",
        "# # Function to process the prediction\n",
        "# def process_prediction(input_context):\n",
        "#   try:\n",
        "#     model_path = \"/content/drive/MyDrive/Whos AI/Turing Bench/author_attribution_model.pt\"\n",
        "#     classifier_path = \"/content/drive/MyDrive/Whos AI/Turing Bench/whosai_classifier_0.joblib\"\n",
        "#     inference_helper = InferenceHelper(model_path, classifier_path)\n",
        "#     predictions = inference_helper.predict_single(input_context)\n",
        "#     return predictions[\"multiclass_prediction\"]\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error during prediction: {e}\")\n",
        "#     return f\"Error during prediction: {e}\"\n",
        "\n",
        "# # Create the Gradio interface\n",
        "# def create_gradio_interface():\n",
        "#     llm_names = df[\"label\"].unique()\n",
        "\n",
        "#     # Text boxes\n",
        "#     input_textbox = gr.Textbox(label=\"Context\", interactive=False, lines=5)\n",
        "#     prediction_box = gr.Textbox(label=\"Your Prediction\", interactive=True)\n",
        "#     output_box = gr.Textbox(label=\"Output\")\n",
        "\n",
        "#     # Interface Layout\n",
        "#     with gr.Blocks() as app:\n",
        "#         with gr.Row():\n",
        "#             gr.Markdown(\"## LLM Selector and Predictor\")\n",
        "\n",
        "#         with gr.Row():\n",
        "#             for llm_name in llm_names:\n",
        "#                 # Correctly pass the current value of llm_name\n",
        "#                 gr.Button(llm_name).click(\n",
        "#                     fn=lambda llm=llm_name: get_random_sentence(llm),\n",
        "#                     inputs=None,\n",
        "#                     outputs=input_textbox\n",
        "#                 )\n",
        "\n",
        "#         with gr.Row():\n",
        "#             gr.Markdown(\"### Enter Your Prediction\")\n",
        "#             prediction_box.render()\n",
        "\n",
        "#         with gr.Row():\n",
        "#             submit_button = gr.Button(\"Submit Prediction\")\n",
        "#             submit_button.click(\n",
        "#                 fn=process_prediction, inputs=input_textbox, outputs=output_box\n",
        "#             )\n",
        "\n",
        "#         with gr.Row():\n",
        "#             gr.Markdown(\"### Model Output\")\n",
        "#             output_box.render()\n",
        "\n",
        "#     return app\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app = create_gradio_interface()\n",
        "#     app.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc62tMjqGV9B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtzht6r8epp7",
        "outputId": "24fbf24e-c89d-40b7-d326-b36be28f0eec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/MyDrive': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sySW7g7FLniT",
        "outputId": "251aa024-1e68-4d3c-9160-c09793e4b4fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/119.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.9\n"
          ]
        }
      ],
      "source": [
        "!pip install dill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEOA296TlO8a"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlG2P0U9Or_f",
        "outputId": "69971bc3-923e-4eb5-afb7-ab96ed8e6039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu121\n",
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and classifier loaded successfully!\n",
            "\n",
            "Enter a text to classify (or 'quit' to exit):\n",
            "quit\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch import nn\n",
        "from io import BytesIO\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "# import os\n",
        "# import threading\n",
        "\n",
        "# os.environ['OPENBLAS_NUM_THREADS'] = '64'\n",
        "# os.environ['OMP_NUM_THREADS'] = '64'\n",
        "# threading.stack_size(2**26)\n",
        "\n",
        "class WhosAIClassifier:\n",
        "    def __init__(self, embedder, tokenizer, device):\n",
        "        self.embedder = embedder\n",
        "        self.multiclass_centroids = None\n",
        "        self.binary_centroids = None\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def _get_embeddings(self, texts, batch_size=32):\n",
        "        embeddings_list = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            encoding = self.tokenizer(batch_texts,\n",
        "                                    truncation=True,\n",
        "                                    max_length=512,\n",
        "                                    padding='max_length',\n",
        "                                    return_tensors='pt')\n",
        "            input_ids = encoding['input_ids'].to(self.device)\n",
        "            attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.embedder(input_ids, attention_mask).detach().cpu().numpy()\n",
        "            embeddings_list.append(embeddings)\n",
        "\n",
        "        return np.concatenate(embeddings_list, axis=0)\n",
        "\n",
        "    def fit(self, texts, labels, batch_size=32):\n",
        "        binary_labels = [1 if label == 'human' else 0 for label in labels]\n",
        "        embeddings = self._get_embeddings(texts, batch_size)\n",
        "        self.multiclass_centroids = NearestCentroid().fit(embeddings, labels)\n",
        "        self.binary_centroids = NearestCentroid().fit(embeddings, binary_labels)\n",
        "        return self\n",
        "\n",
        "    def predict(self, texts, batch_size=32):\n",
        "        embeddings = self._get_embeddings(texts, batch_size)\n",
        "        multiclass_predictions = self.multiclass_centroids.predict(embeddings)\n",
        "        binary_predictions = self.binary_centroids.predict(embeddings)\n",
        "        return multiclass_predictions, binary_predictions\n",
        "\n",
        "    def save(self, path=\"classifier.joblib\"):\n",
        "        joblib.dump(self, path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path=\"classifier.joblib\"):\n",
        "        return joblib.load(path)\n",
        "\n",
        "class AuthorAttributionModel(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
        "        super().__init__()\n",
        "        from transformers import AutoModel\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        embeddings = self.projection(embeddings)\n",
        "        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "        return embeddings\n",
        "\n",
        "class InferenceHelper:\n",
        "    def __init__(self, model_path=\"author_attribution_model.pt\", classifier_path=\"whosai_classifier.joblib\"):\n",
        "        # Determine the best available device\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Create a flexible map_location function\n",
        "        def flexible_map_location(storage, location):\n",
        "            # This will map all previous device storages to the current available device\n",
        "            return storage.cuda(self.device.index) if self.device.type == 'cuda' else storage.cpu()\n",
        "\n",
        "        # Load the model with flexible device mapping\n",
        "        self.model = AuthorAttributionModel()\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=flexible_map_location, weights_only=True))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.classifier = joblib.load(classifier_path)\n",
        "        self.classifier.embedder = self.model\n",
        "        self.classifier.device = self.device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "    def get_embedding(self, text):\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model(input_ids, attention_mask)\n",
        "\n",
        "        return embedding.cpu().numpy()\n",
        "\n",
        "    def predict_single(self, text):\n",
        "        embedding = self.get_embedding(text)\n",
        "        multiclass_pred = self.classifier.multiclass_centroids.predict(embedding)\n",
        "        binary_pred = self.classifier.binary_centroids.predict(embedding)\n",
        "        binary_label = \"human\" if binary_pred[0] == 1 else \"non-human\"\n",
        "\n",
        "        return {\n",
        "            \"multiclass_prediction\": multiclass_pred[0],\n",
        "            \"binary_prediction\": binary_label,\n",
        "            \"embedding\": embedding[0]\n",
        "        }\n",
        "\n",
        "\n",
        "def Inference(model_path, classifier_path):\n",
        "    try:\n",
        "        inference = InferenceHelper(model_path, classifier_path)\n",
        "        print(\"Model and classifier loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model or classifier: {e}\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nEnter a text to classify (or 'quit' to exit):\")\n",
        "        text = input()\n",
        "\n",
        "        if text.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            predictions = inference.predict_single(text)\n",
        "            print(\"\\nPredictions:\")\n",
        "            print(f\"Detailed class: {predictions['multiclass_prediction']}\")\n",
        "            print(f\"Binary class: {predictions['binary_prediction']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error making prediction: {e}\")\n",
        "\n",
        "def evaluate_whosai(model_path, classifier_path, test_csv_path, batch_size=32):\n",
        "    inference_helper = InferenceHelper(model_path, classifier_path)\n",
        "    print(\"Model and classifier loaded successfully!\")\n",
        "    print(\"Loading test data...\")\n",
        "    test_data = pd.read_csv(test_csv_path)\n",
        "    texts = test_data['Generation'].tolist()\n",
        "    true_labels = test_data['label'].tolist()\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    multiclass_pred, binary_pred = inference_helper.classifier.predict(texts, batch_size=batch_size)\n",
        "    binary_pred_labels = ['human' if pred == 1 else 'non-human' for pred in binary_pred]\n",
        "    binary_true_labels = ['human' if label == 'human' else 'non-human' for label in true_labels]\n",
        "\n",
        "    print(\"\\nMulticlass Classification Report:\")\n",
        "    print(classification_report(true_labels, multiclass_pred))\n",
        "\n",
        "    print(\"\\nBinary Classification Report (Human vs Non-Human):\")\n",
        "    print(classification_report(binary_true_labels, binary_pred_labels))\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Text': texts,\n",
        "        'True_Label': true_labels,\n",
        "        'Predicted_Multiclass': multiclass_pred,\n",
        "        'Predicted_Binary': binary_pred_labels\n",
        "    })\n",
        "    results_df.to_csv('evaluation_results.csv', index=False)\n",
        "    print(\"\\nDetailed results saved to 'evaluation_results.csv'\")\n",
        "\n",
        "def Test():\n",
        "    try:\n",
        "        # model_path, classifier_path, test_path = \"author_attribution_model.pt\", \"whosai_classifier.joblib\", \"test.csv\"\n",
        "        # evaluate_whosai(model_path, classifier_path, test_path, batch_size=32)\n",
        "        model_path, classifier_path = \"/content/drive/MyDrive/Whos AI/Turing Bench/author_attribution_model.pt\", \"/content/drive/MyDrive/Whos AI/Turing Bench/whosai_classifier_0.joblib\"\n",
        "        Inference(model_path, classifier_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Test()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
